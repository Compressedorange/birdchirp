import tensorflow as tf
from tensorflow.keras.models import load_model
import numpy as np
import librosa
import soundfile as sf
import matplotlib.pyplot as plt
from IPython.display import Audio, display
import os
import shutil
import time
# Constants
SAMPLE_RATE = 16000
CLIP_SAMPLES = 16384
NOISE_DIM = 100
BATCH_SIZE = 128
EPOCHS = 10000
DATASET_PATH = "/kaggle/input/birbchirp/wav_files"
LAMBDA_GP = 10.0  # Gradient penalty coefficient

#alters training data
def center_chirp(signal, clip_samples=CLIP_SAMPLES):
    onset_env = librosa.onset.onset_strength(y=signal)
    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, backtrack=True)
    
    if len(onset_frames) == 0:
        return signal[:clip_samples]  # fallback

    onset_sample = librosa.frames_to_samples(onset_frames[0])
    half_clip = clip_samples // 2

    start = max(0, onset_sample - half_clip)
    end = start + clip_samples

    clipped = signal[start:end]
    if len(clipped) < clip_samples:
        clipped = np.pad(clipped, (0, clip_samples - len(clipped)))
    return clipped

def augment_signal(signal, sample_rate=16384):
    # Random pitch shift
    if np.random.rand() < 0.5:
        signal = librosa.effects.pitch_shift(signal, sr=sample_rate, n_steps=np.random.uniform(-2, 2))

    # Random time stretch
    if np.random.rand() < 0.5:
        signal = librosa.effects.time_stretch(signal, rate=np.random.uniform(0.8, 1.2))
        signal = librosa.util.fix_length(data=signal, size=CLIP_SAMPLES)

    # Random shift
    if np.random.rand() < 0.5:
        shift = int(np.random.uniform(-1000, 1000))
        signal = np.roll(signal, shift)

    # Add random background noise
    if np.random.rand() < 0.3:
        noise = np.random.normal(0, 0.005, size=signal.shape)
        signal = signal + noise

    return signal

def invert_spectrogram(spec, sample_rate=16384, n_fft=1024, hop_length=256, n_iter=60):
    # Un-normalize if needed
    spec = np.squeeze(spec)
    spec = spec * 80 - 80  # From [0,1] back to [-80, 0] dB (approx)

    # Convert back to linear power
    power_spec = librosa.db_to_power(spec)

    # Reconstruct waveform with Griffin-Lim
    waveform = librosa.feature.inverse.mel_to_audio(
        power_spec,
        sr=sample_rate,
        n_fft=n_fft,
        hop_length=hop_length,
        n_iter=n_iter
    )
    return waveform

def load_audio_files(directory, sample_rate=16384, clip_samples=16384, n_fft=1024, hop_length=256, n_mels=64, augment=True):
    spectrograms = []
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        signal, sr = librosa.load(filepath, sr=sample_rate, mono=True)

        if len(signal) < clip_samples:
            signal = np.pad(signal, (0, clip_samples - len(signal)))
        elif len(signal) > clip_samples:
            signal = signal[:clip_samples]

        signal = center_chirp(signal, clip_samples)

        if augment:
            signal = augment_signal(signal, sample_rate)

        # Convert to log-Mel spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=signal,
            sr=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels
        )
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # dB scale

        # Normalize to [0, 1]
        log_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())

        spectrograms.append(log_mel_spec)

    spectrograms = np.array(spectrograms)

    # Resize to (64, 64)
    resized_specs = []
    for spec in spectrograms:
        spec_resized = tf.image.resize(spec[..., np.newaxis], size=(64, 64)).numpy()
        resized_specs.append(spec_resized)
    
    return np.array(resized_specs)

# Example usage
audio_data = load_audio_files(DATASET_PATH, augment=True)

# Create dataset and shuffle
dataset = tf.data.Dataset.from_tensor_slices(audio_data)
dataset = dataset.shuffle(buffer_size=len(audio_data))  # Full shuffle
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)

def build_generator():
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(NOISE_DIM,)),
        tf.keras.layers.Dense(8 * 8 * 256, use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),

        tf.keras.layers.Reshape((8, 8, 256)),  # Shape: (batch, 8, 8, 256)

        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),

        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),

        tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'),
        # Output shape: (batch, 64, 64, 1)
    ])
    return model

def build_discriminator():
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(64, 64, 1)),

        tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1),
    ])
    return model
def save_audio_sample(generator, epoch):
    noise = tf.random.normal([1, NOISE_DIM])
    generated_spec = generator(noise, training=False).numpy()
    
    waveform = invert_spectrogram(generated_spec)
    sf.write(f"generated_sample_epoch_{epoch}.wav", waveform, 16384)

def save_generator(generator, epoch):
    save_dir = '/kaggle/working/generators'  # Temp space
    os.makedirs(save_dir, exist_ok=True)

    g_path = os.path.join(save_dir, f'generator_epoch_{epoch}.keras')
    generator.save(g_path)

    # Copy to output directory so Kaggle keeps it
    gen_output_dir = '/kaggle/outputs/generators'
    os.makedirs(gen_output_dir, exist_ok=True)
    shutil.copy(g_path, gen_output_dir)

    print(f"Generator saved to {g_path} and copied to output.")

def save_discriminator(discriminator, epoch):
    save_dir = '/kaggle/working/discriminators'  # Temp space
    os.makedirs(save_dir, exist_ok=True)
    d_path = os.path.join(save_dir, f'discriminator_epoch_{epoch}.keras')
    discriminator.save(d_path)
    # Copy to output directory so Kaggle keeps it
    disc_output_dir = '/kaggle/outputs/discriminators'
    os.makedirs(disc_output_dir, exist_ok=True)
    shutil.copy(d_path, disc_output_dir)

    print(f"Discriminator saved to {d_path} and copied to output.")

def gradient_penalty(discriminator, real_audio, fake_audio):
    """Calculates the gradient penalty for WGAN-GP."""
    batch_size = tf.shape(real_audio)[0]
    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)

    interpolated = alpha * tf.cast(real_audio, tf.float32) + (1 - alpha) * tf.cast(fake_audio, tf.float32)
    with tf.GradientTape() as gp_tape:
        gp_tape.watch(interpolated)
        pred = discriminator(interpolated, training=True)

    grads = gp_tape.gradient(pred, interpolated)
    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]) + 1e-12)
    gp = tf.reduce_mean((norm - 1.0) ** 2)
    return gp

def discriminator_loss(real_output, fake_output, gp):
    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + LAMBDA_GP * gp

def generator_loss(fake_output):
    return -tf.reduce_mean(fake_output)

def train(dataset, generator, discriminator, noise_dim=NOISE_DIM, batch_size=BATCH_SIZE, epochs=EPOCHS):
    global gen_loss_history
    global disc_loss_history
    gen_loss_history = []
    disc_loss_history = []

    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.9)
    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)

    @tf.function
    def train_step(real_spec):
        noise = tf.random.normal([batch_size, noise_dim])
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            generated_spec = generator(noise, training=True)
            real_output = discriminator(real_spec, training=True)
            fake_output = discriminator(generated_spec, training=True)
    
            gp = gradient_penalty(discriminator, real_spec, generated_spec)
    
            gen_loss = generator_loss(fake_output)
            disc_loss = discriminator_loss(real_output, fake_output, gp)
        
            gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)
            disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
        
            generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))
            discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))
    
            return gen_loss, disc_loss

    for epoch in range(epochs):
        start_time = time.time()
        print(f"\nEpoch {epoch + 1}/{epochs}")

        for batch in dataset:
            gen_loss, disc_loss = train_step(batch)
            gen_loss_history.append(float(gen_loss))
            disc_loss_history.append(float(disc_loss))

        print(f"  Generator loss: {gen_loss_history[-1]:.4f}")
        print(f"  Discriminator loss: {disc_loss_history[-1]:.4f}")
        print(f"  Epoch time: {time.time() - start_time:.2f} sec")

        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:
            save_generator(generator, epoch + 1)
            save_discriminator(discriminator, epoch + 1)
            save_audio_sample(generator, epoch + 1)

def plot_losses(gen_loss_history, disc_loss_history):
    plt.figure(figsize=(10, 5))
    plt.plot(gen_loss_history, label='Generator Loss')
    plt.plot(disc_loss_history, label='Discriminator Loss')
    plt.legend()
    plt.title('GAN Training Losses')
    plt.xlabel('Batch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()

def generate_and_play(generator):
    noise = tf.random.normal([1, NOISE_DIM])
    generated_spec = generator(noise, training=False).numpy()
    waveform = invert_spectrogram(generated_spec)
    return ipd.Audio(waveform, rate=SAMPLE_RATE)

def show_generated_spec(generator):
    noise = tf.random.normal([1, NOISE_DIM])
    spec = generator(noise, training=False).numpy()[0, :, :, 0]
    plt.imshow(spec, aspect='auto', origin='lower', cmap='magma')
    plt.title("Generated Spectrogram")
    plt.colorbar()
    plt.show()

with strategy.scope():
    generator = build_generator()
    discriminator = build_discriminator()
    
    train(dataset, generator, discriminator)

plot_losses(gen_loss_history, disc_loss_history)

# Generate and listen to a sample after training
audio = generate_and_play(generator)
display(audio)
